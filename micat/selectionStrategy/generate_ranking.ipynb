{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b98d15e-d51a-4133-a330-0699dc7d8132",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from autorank import autorank, create_report, plot_stats \n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "algorithms = [\"Adam\", \"Approx_GAP\", \"Beta_cd\", \"MICAT\", \"MAML\"]\n",
    "datasets = [\"algebra\", \"assist0910\", \"math2\"]\n",
    "subalgos = [\"IMPACT\", \"NCDM\"]\n",
    "metrics = [\"mi_acc\", \"mi_auc\", \"meta_doa\"]\n",
    "pattern = re.compile(r\"CAT_launch_(?P<dataset>[^_]+)_(?P<subalgo>IMPACT|NCDM)_(?P<algorithm>[^_]+)_\\d+_all_results\\.json\")\n",
    "output_dir = \"cd_plots\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Collect metric values across all valid datasets/subalgos/folds/steps\n",
    "results = {m: {a: [] for a in algorithms} for m in metrics}\n",
    "files = glob.glob(\"CAT_launch_*_IMPACT_*_all_results.json\") + \\\n",
    "        glob.glob(\"CAT_launch_*_NCDM_*_all_results.json\")\n",
    "print(f\"[INFO] Found {len(files)} candidate result files.\")\n",
    "\n",
    "for file in files:\n",
    "    print(f\"[DEBUG] Processing file: {file}\")\n",
    "    m = pattern.match(os.path.basename(file))\n",
    "    if not m:\n",
    "        print(f\"[WARN] Filename does not match expected pattern: {file}\")\n",
    "        continue\n",
    "    ds, subalgo, algo = m[\"dataset\"], m[\"subalgo\"], m[\"algorithm\"]\n",
    "    print(f\"[DEBUG] Parsed: dataset={ds}, subalgo={subalgo}, algo={algo}\")\n",
    "    if ds not in datasets or algo not in algorithms:\n",
    "        print(f\"[DEBUG] Skipping file due to unmatched dataset/algo: {ds}, {algo}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with open(file, \"r\") as f:\n",
    "            folds = json.load(f)\n",
    "        print(f\"[DEBUG] Loaded {len(folds)} folds\")\n",
    "        for fold_idx, (pred, meta) in enumerate(folds):\n",
    "            for step in pred:\n",
    "                for metric in metrics:\n",
    "                    value = pred[step].get(metric)\n",
    "                    if value is None:\n",
    "                        value = meta[step].get(metric)\n",
    "                    if value is not None and not np.isnan(value):\n",
    "                        results[metric][algo].append(value)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to process {file}: {e}\")\n",
    "\n",
    "# Plotting using autorank\n",
    "for metric in metrics:\n",
    "    data = results[metric]\n",
    "    filtered_data = {k: v for k, v in data.items() if len(v) > 0}\n",
    "\n",
    "    print(f\"[INFO] Evaluating metric '{metric}' with {len(filtered_data)} algorithms having data.\")\n",
    "    for algo, vals in filtered_data.items():\n",
    "        print(f\"  [DEBUG] {algo} → {len(vals)} values\")\n",
    "\n",
    "    if len(filtered_data) < 2:\n",
    "        print(f\"[WARN] Not enough algorithms with data to plot '{metric}' — skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Equalize lengths by trimming to shortest list\n",
    "    min_len = min(len(v) for v in filtered_data.values())\n",
    "    print(f\"[INFO] Trimming all data to {min_len} samples\")\n",
    "    trimmed_data = {k: v[:min_len] for k, v in filtered_data.items()}\n",
    "    df = pd.DataFrame(trimmed_data)\n",
    "\n",
    "    # Statistical test + CD plot\n",
    "    try:\n",
    "        result = autorank(df, alpha=0.05, verbose=True)\n",
    "        print(f\"[DEBUG] autorank result object for metric '{metric}':\")\n",
    "        from pprint import pprint\n",
    "        print(\"[DEBUG] autorank result object (pretty print):\")\n",
    "        pprint(result)\n",
    "\n",
    "\n",
    "        report = create_report(result)\n",
    "        if report is None:\n",
    "            print(f\"[WARN] create_report() failed for metric '{metric}', writing summary manually.\")\n",
    "            summary_path = os.path.join(output_dir, f\"report_{metric}.txt\")\n",
    "            with open(summary_path, \"w\") as f:\n",
    "                f.write(\"Manual report fallback:\\n\")\n",
    "                f.write(str(result.df_res))\n",
    "                f.write(\"\\n\\nRanks:\\n\")\n",
    "                f.write(str(result.ranks))\n",
    "                f.write(\"\\n\\nRanked groups:\\n\")\n",
    "                f.write(str(result.rankdf))\n",
    "            continue\n",
    "\n",
    "        \n",
    "        # Sauvegarde du rapport\n",
    "        try:\n",
    "            report_path = os.path.join(output_dir, f\"report_{metric}.txt\")\n",
    "            with open(report_path, \"w\") as f:\n",
    "                try:\n",
    "                    report = create_report(result)\n",
    "                    if not isinstance(report, str):\n",
    "                        raise TypeError(\"create_report() did not return a string\")\n",
    "                    f.write(report)\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] create_report() failed for metric '{metric}', writing summary manually.\")\n",
    "                    from pprint import pprint\n",
    "                    f.write(\"Fallback summary:\\n\")\n",
    "                    pprint(result, stream=f)\n",
    "            print(f\"[✔] Saved statistical report: {report_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to save report for {metric}: {e}\")\n",
    "\n",
    "\n",
    "        try:\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            # Correction manuelle pour contourner le bug de df_res manquant\n",
    "            plot_stats(result.rankdf, cd=result.cd, reverse=True)\n",
    "            plt.title(f\"Critical Difference Diagram — {metric}\")\n",
    "            plt.tight_layout()\n",
    "            fig_path = os.path.join(output_dir, f\"cd_diagram_{metric}.png\")\n",
    "            plt.savefig(fig_path)\n",
    "            plt.close()\n",
    "            print(f\"[✔] Saved CD diagram: {fig_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to generate CD plot for {metric}: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to generate CD plot for {metric}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.7.0_py3.12.10",
   "language": "python",
   "name": "module-conda-env-pytorch-gpu-2.7.0_py3.12.10"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
