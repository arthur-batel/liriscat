Loading pytorch-gpu/py3/2.7.0
  Loading requirement: gcc/11.4.1 cuda/12.6.3 nccl/2.25.1-1-cuda
    cudnn/9.5.1.17-cuda openmpi/4.1.6-cuda intel-mkl/2020.4 magma/2.9.0-cuda
    sox/14.4.2 hdf5/1.12.0-mpi-cuda libjpeg-turbo/2.1.3 ffmpeg/6.1.1
    openjdk/11.0.2
/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/selectionStrategy/random.py:30: UserWarning: get_params() Notimplemented
  warnings.warn('get_params() Notimplemented')
  0%|                                                                                                                                                                                                                                    | 0/100 [00:00<?, ?it/s]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
  1%|██▏                                                                                                                                                                                                                       | 1/100 [00:47<1:18:42, 47.70s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
  2%|████▎                                                                                                                                                                                                                     | 2/100 [01:30<1:13:11, 44.81s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
  3%|██████▌                                                                                                                                                                                                                   | 3/100 [02:14<1:11:44, 44.38s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
  4%|████████▋                                                                                                                                                                                                                 | 4/100 [02:57<1:09:58, 43.74s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
  5%|██████████▉                                                                                                                                                                                                               | 5/100 [03:39<1:08:20, 43.16s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
  6%|█████████████                                                                                                                                                                                                             | 6/100 [04:21<1:07:23, 43.02s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
  7%|███████████████▎                                                                                                                                                                                                          | 7/100 [05:03<1:05:42, 42.39s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
  8%|█████████████████▍                                                                                                                                                                                                        | 8/100 [05:44<1:04:29, 42.06s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
  9%|███████████████████▌                                                                                                                                                                                                      | 9/100 [06:25<1:03:25, 41.82s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 10%|█████████████████████▋                                                                                                                                                                                                   | 10/100 [07:06<1:02:08, 41.43s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 11%|███████████████████████▊                                                                                                                                                                                                 | 11/100 [07:47<1:01:07, 41.21s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 12%|██████████████████████████▎                                                                                                                                                                                                | 12/100 [08:27<59:59, 40.90s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 13%|████████████████████████████▍                                                                                                                                                                                              | 13/100 [09:06<58:47, 40.55s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 14%|██████████████████████████████▋                                                                                                                                                                                            | 14/100 [09:46<57:47, 40.31s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 15%|████████████████████████████████▊                                                                                                                                                                                          | 15/100 [10:26<56:46, 40.08s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 16%|███████████████████████████████████                                                                                                                                                                                        | 16/100 [11:05<55:44, 39.81s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 17%|█████████████████████████████████████▏                                                                                                                                                                                     | 17/100 [11:44<54:34, 39.45s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 18%|███████████████████████████████████████▍                                                                                                                                                                                   | 18/100 [12:23<53:52, 39.42s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 19%|█████████████████████████████████████████▌                                                                                                                                                                                 | 19/100 [13:02<53:01, 39.28s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 20%|███████████████████████████████████████████▊                                                                                                                                                                               | 20/100 [13:40<51:48, 38.86s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 21%|█████████████████████████████████████████████▉                                                                                                                                                                             | 21/100 [14:18<50:58, 38.71s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 22%|████████████████████████████████████████████████▏                                                                                                                                                                          | 22/100 [14:56<50:04, 38.51s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 23%|██████████████████████████████████████████████████▎                                                                                                                                                                        | 23/100 [15:34<49:09, 38.31s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 24%|████████████████████████████████████████████████████▌                                                                                                                                                                      | 24/100 [16:12<48:21, 38.18s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 25%|██████████████████████████████████████████████████████▊                                                                                                                                                                    | 25/100 [16:50<47:37, 38.10s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 26%|████████████████████████████████████████████████████████▉                                                                                                                                                                  | 26/100 [17:27<46:46, 37.93s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 27%|███████████████████████████████████████████████████████████▏                                                                                                                                                               | 27/100 [18:04<45:47, 37.64s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 28%|█████████████████████████████████████████████████████████████▎                                                                                                                                                             | 28/100 [18:42<45:04, 37.56s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 29%|███████████████████████████████████████████████████████████████▌                                                                                                                                                           | 29/100 [19:19<44:14, 37.38s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 30%|█████████████████████████████████████████████████████████████████▋                                                                                                                                                         | 30/100 [19:55<43:25, 37.21s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 31%|███████████████████████████████████████████████████████████████████▉                                                                                                                                                       | 31/100 [20:31<42:21, 36.83s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 32%|██████████████████████████████████████████████████████████████████████                                                                                                                                                     | 32/100 [21:08<41:33, 36.68s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 33%|████████████████████████████████████████████████████████████████████████▎                                                                                                                                                  | 33/100 [21:44<40:57, 36.68s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 34%|██████████████████████████████████████████████████████████████████████████▍                                                                                                                                                | 34/100 [22:21<40:19, 36.67s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 35%|████████████████████████████████████████████████████████████████████████████▋                                                                                                                                              | 35/100 [22:58<39:42, 36.66s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 36%|██████████████████████████████████████████████████████████████████████████████▊                                                                                                                                            | 36/100 [23:34<38:53, 36.46s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 37%|█████████████████████████████████████████████████████████████████████████████████                                                                                                                                          | 37/100 [24:10<38:10, 36.36s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 38%|███████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                       | 38/100 [24:45<37:17, 36.08s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 39%|█████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                     | 39/100 [25:22<36:52, 36.28s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 40%|███████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                   | 40/100 [25:57<36:00, 36.01s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 41%|█████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                 | 41/100 [26:33<35:20, 35.94s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 42%|███████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                               | 42/100 [27:10<34:57, 36.17s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 43%|██████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                            | 43/100 [27:45<34:07, 35.93s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 44%|████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                          | 44/100 [28:21<33:33, 35.96s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 45%|██████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                        | 45/100 [28:56<32:40, 35.65s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 46%|████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                      | 46/100 [29:31<31:46, 35.30s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 47%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                    | 47/100 [30:06<31:10, 35.29s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 48%|█████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                  | 48/100 [30:41<30:29, 35.18s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 49%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                               | 49/100 [31:16<29:52, 35.14s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 50%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                             | 50/100 [31:51<29:16, 35.14s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 51%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                           | 51/100 [32:26<28:37, 35.05s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 52%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                         | 52/100 [33:01<28:00, 35.00s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 53%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                       | 53/100 [33:36<27:29, 35.09s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 53%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                       | 53/100 [34:10<30:18, 38.70s/it]
  0%|                                                                                                                                                                                                                                     | 0/25 [00:00<?, ?it/s]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
  4%|████████▊                                                                                                                                                                                                                    | 1/25 [00:00<00:04,  5.12it/s]  8%|█████████████████▋                                                                                                                                                                                                           | 2/25 [00:00<00:06,  3.43it/s] 12%|██████████████████████████▌                                                                                                                                                                                                  | 3/25 [00:00<00:05,  4.31it/s] 16%|███████████████████████████████████▎                                                                                                                                                                                         | 4/25 [00:00<00:04,  4.74it/s] 20%|████████████████████████████████████████████▏                                                                                                                                                                                | 5/25 [00:01<00:04,  4.92it/s] 24%|█████████████████████████████████████████████████████                                                                                                                                                                        | 6/25 [00:01<00:03,  4.91it/s] 28%|█████████████████████████████████████████████████████████████▉                                                                                                                                                               | 7/25 [00:01<00:03,  4.79it/s] 32%|██████████████████████████████████████████████████████████████████████▋                                                                                                                                                      | 8/25 [00:01<00:03,  4.60it/s] 36%|███████████████████████████████████████████████████████████████████████████████▌                                                                                                                                             | 9/25 [00:02<00:04,  3.81it/s] 40%|████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                    | 10/25 [00:02<00:04,  3.36it/s] 44%|████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                           | 11/25 [00:02<00:04,  3.07it/s] 48%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                  | 12/25 [00:03<00:05,  2.41it/s] 52%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                         | 13/25 [00:03<00:04,  2.40it/s] 56%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                | 14/25 [00:04<00:04,  2.37it/s] 60%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                        | 15/25 [00:04<00:04,  2.32it/s] 64%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                               | 16/25 [00:05<00:04,  1.98it/s] 68%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                      | 17/25 [00:05<00:04,  2.00it/s] 72%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                             | 18/25 [00:06<00:03,  1.88it/s] 76%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                    | 19/25 [00:07<00:03,  1.60it/s] 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                            | 20/25 [00:08<00:03,  1.57it/s] 84%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                   | 21/25 [00:08<00:02,  1.56it/s] 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                          | 22/25 [00:09<00:01,  1.54it/s] 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                 | 23/25 [00:10<00:01,  1.38it/s] 96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏        | 24/25 [00:10<00:00,  1.40it/s]100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:11<00:00,  1.28it/s]100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:11<00:00,  2.10it/s]
