Loading pytorch-gpu/py3/2.7.0
  Loading requirement: gcc/11.4.1 cuda/12.6.3 nccl/2.25.1-1-cuda
    cudnn/9.5.1.17-cuda openmpi/4.1.6-cuda intel-mkl/2020.4 magma/2.9.0-cuda
    sox/14.4.2 hdf5/1.12.0-mpi-cuda libjpeg-turbo/2.1.3 ffmpeg/6.1.1
    openjdk/11.0.2
/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/selectionStrategy/random.py:30: UserWarning: get_params() Notimplemented
  warnings.warn('get_params() Notimplemented')
  0%|                                                                                                                                                                                                                                    | 0/100 [00:00<?, ?it/s]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
  1%|██▏                                                                                                                                                                                                                       | 1/100 [00:43<1:11:02, 43.06s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
  2%|████▎                                                                                                                                                                                                                     | 2/100 [01:17<1:02:16, 38.13s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
  3%|██████▌                                                                                                                                                                                                                     | 3/100 [01:50<57:54, 35.81s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
  4%|████████▊                                                                                                                                                                                                                   | 4/100 [02:21<54:22, 33.98s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
  5%|███████████                                                                                                                                                                                                                 | 5/100 [02:51<51:31, 32.54s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
  6%|█████████████▏                                                                                                                                                                                                              | 6/100 [03:21<49:30, 31.60s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
  7%|███████████████▍                                                                                                                                                                                                            | 7/100 [03:50<47:29, 30.63s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
  8%|█████████████████▌                                                                                                                                                                                                          | 8/100 [04:18<45:47, 29.86s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
  9%|███████████████████▊                                                                                                                                                                                                        | 9/100 [04:46<44:16, 29.19s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 10%|█████████████████████▉                                                                                                                                                                                                     | 10/100 [05:13<42:59, 28.66s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 11%|████████████████████████                                                                                                                                                                                                   | 11/100 [05:41<41:55, 28.26s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 12%|██████████████████████████▎                                                                                                                                                                                                | 12/100 [06:07<40:46, 27.81s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 13%|████████████████████████████▍                                                                                                                                                                                              | 13/100 [06:34<39:38, 27.34s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 14%|██████████████████████████████▋                                                                                                                                                                                            | 14/100 [07:00<38:35, 26.93s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 15%|████████████████████████████████▊                                                                                                                                                                                          | 15/100 [07:25<37:34, 26.53s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 16%|███████████████████████████████████                                                                                                                                                                                        | 16/100 [07:51<36:41, 26.21s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 17%|█████████████████████████████████████▏                                                                                                                                                                                     | 17/100 [08:16<35:55, 25.97s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 18%|███████████████████████████████████████▍                                                                                                                                                                                   | 18/100 [08:42<35:19, 25.85s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 19%|█████████████████████████████████████████▌                                                                                                                                                                                 | 19/100 [09:07<34:50, 25.81s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 20%|███████████████████████████████████████████▊                                                                                                                                                                               | 20/100 [09:33<34:07, 25.60s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 21%|█████████████████████████████████████████████▉                                                                                                                                                                             | 21/100 [09:57<33:27, 25.41s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 22%|████████████████████████████████████████████████▏                                                                                                                                                                          | 22/100 [10:22<32:48, 25.24s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 23%|██████████████████████████████████████████████████▎                                                                                                                                                                        | 23/100 [10:47<32:10, 25.07s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 24%|████████████████████████████████████████████████████▌                                                                                                                                                                      | 24/100 [11:11<31:32, 24.90s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 25%|██████████████████████████████████████████████████████▊                                                                                                                                                                    | 25/100 [11:36<30:55, 24.73s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 26%|████████████████████████████████████████████████████████▉                                                                                                                                                                  | 26/100 [12:00<30:27, 24.70s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 27%|███████████████████████████████████████████████████████████▏                                                                                                                                                               | 27/100 [12:25<30:00, 24.66s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 28%|█████████████████████████████████████████████████████████████▎                                                                                                                                                             | 28/100 [12:50<29:36, 24.67s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 29%|███████████████████████████████████████████████████████████████▌                                                                                                                                                           | 29/100 [13:14<29:00, 24.52s/it]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
 29%|███████████████████████████████████████████████████████████████▌                                                                                                                                                           | 29/100 [13:38<33:23, 28.22s/it]
  0%|                                                                                                                                                                                                                                     | 0/16 [00:00<?, ?it/s]/lustre/fsn1/projects/rech/enh/unv34ei/MICAT/micat/meta_models/meta_models.py:70: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /lustre/fshomisc/sup/hpe/src/pub/miniforge/24.11.3/pytorch-gpu-2.7.0+py3.12.10+cuda-12.6/pytorch-2.7.0/build/aten/src/ATen/core/TensorBody.h:489.)
  if getattr(p, "grad", None) is not None:
  6%|█████████████▊                                                                                                                                                                                                               | 1/16 [00:00<00:03,  4.51it/s] 12%|███████████████████████████▋                                                                                                                                                                                                 | 2/16 [00:00<00:04,  3.00it/s] 19%|█████████████████████████████████████████▍                                                                                                                                                                                   | 3/16 [00:00<00:03,  3.54it/s] 25%|███████████████████████████████████████████████████████▎                                                                                                                                                                     | 4/16 [00:01<00:03,  3.68it/s] 31%|█████████████████████████████████████████████████████████████████████                                                                                                                                                        | 5/16 [00:01<00:03,  3.12it/s] 38%|██████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                          | 6/16 [00:01<00:03,  2.76it/s] 44%|████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                            | 7/16 [00:02<00:03,  2.51it/s] 50%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                              | 8/16 [00:02<00:03,  2.32it/s] 56%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                | 9/16 [00:03<00:03,  1.76it/s] 62%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                  | 10/16 [00:04<00:03,  1.65it/s] 69%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                    | 11/16 [00:05<00:03,  1.56it/s] 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                       | 12/16 [00:06<00:02,  1.35it/s] 81%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                         | 13/16 [00:07<00:02,  1.27it/s] 88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                           | 14/16 [00:08<00:01,  1.11it/s] 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎             | 15/16 [00:09<00:00,  1.09it/s]100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:10<00:00,  1.01s/it]100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:10<00:00,  1.54it/s]
