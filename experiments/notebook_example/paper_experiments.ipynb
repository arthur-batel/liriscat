{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ff48227",
   "metadata": {},
   "source": [
    "# Paper experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e425abf3",
   "metadata": {},
   "source": [
    "### 1. **Table 2 and 3**: Accuracy and training time tables comparing MICAT performances with competitors \n",
    "\n",
    "\"MICAT\" framework is refered as \"GAP\" in the code, and 'Naive-CAT' as Adam. The first cell set the configuration. **Hyperparameters** must be adapted as indicated, depending on the combination of (Dataset, CDM, Competitor), according to the values contained in file : **'MICAT/experiments/data/CAT_hyperparameters.csv'**, **'MICAT/experiments/data/CDM_hyperparameters.csv'**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8d8a50",
   "metadata": {},
   "source": [
    "### 1.0. Configuration : Choose your combination (Dataset, CDM, Adaptation method), and set the corresponding parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf89a372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU.\n"
     ]
    }
   ],
   "source": [
    "import micat\n",
    "config = micat.utils.generate_eval_config(\n",
    "    #### Change the following values: ####\n",
    "\n",
    "    ### Dataset name :\n",
    "    dataset_name=\"assist0910\",  # ['assist0910', 'math2', 'algebra']\n",
    "    n_query=16, #{'assist0910':16, 'math2':16, 'algebra':25}\n",
    "\n",
    "    ### CDM: \n",
    "    CDM = 'impact', # ['impact','ncdm']\n",
    "    lambda_=7.380681029927064e-05,\n",
    "    learning_rate = 0.001,\n",
    "\n",
    "    ### Competitor:     \n",
    "    meta_trainer='GAP', #['Adam', 'Approx_GAP','MAML','GAP', 'BETA-CD']\n",
    "    num_inner_users_epochs=3,\n",
    "    inner_user_lr= 0.0007838402204978467,\n",
    "    meta_lr=0.05,\n",
    "\n",
    "    #### Fixed hyperpameters ####\n",
    "    load_params=True,\n",
    "    \n",
    "    save_params=False,\n",
    "    esc = 'error',\n",
    "    batch_size = 512,\n",
    "    valid_batch_size = 10000,\n",
    "    num_epochs=100,\n",
    "    eval_freq = 1,\n",
    "    patience = 20,\n",
    "    pred_metrics = [\"mi_acc\",\"rmse\",\"mae\",\"mi_prec\",\"mi_rec\",\"mi_f_b\",\"mi_auc\",\"ma_prec\",\"ma_rec\",\"ma_f_b\"],\n",
    "    profile_metrics = ['meta_doa','pc-er','rm'],\n",
    "    valid_metric= 'rmse',\n",
    ")\n",
    "\n",
    "#### Extra method-specific hyperparameters : \n",
    "\n",
    "## BETA-CD\n",
    "config['kl_weight'] =0.0005\n",
    "\n",
    "## MICAT (GAP)\n",
    "config['learning_users_emb_lr'] = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87880c26",
   "metadata": {},
   "source": [
    "### 1.1. Launch script (to run only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3e567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\"\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":4096:8\"\n",
    "\n",
    "import micat\n",
    "micat.utils.set_seed(0)\n",
    "from micat.dataset import preprocessing_utilities as pu\n",
    "\n",
    "import logging\n",
    "import gc\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import time \n",
    "\n",
    "print(\"PID\", os.getpid(), \"sees CUDA_VISIBLE_DEVICES =\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n",
    "if os.environ.get(\"CUDA_VISIBLE_DEVICES\") is not None:\n",
    "    print(\"→ torch.device is\", torch.cuda.current_device(), torch.cuda.get_device_name(0))\n",
    "    \n",
    "logging.info(f'#### config : {config} ####')\n",
    "\n",
    "concept_map = json.load(open(\n",
    "    f'../datasets/2-preprocessed_data/{config[\"dataset_name\"]}_concept_map.json', 'r'\n",
    "))\n",
    "concept_map = {int(k): [int(x) for x in v] for k, v in concept_map.items()}\n",
    "\n",
    "metadata = json.load(open(\n",
    "    f'../datasets/2-preprocessed_data/{config[\"dataset_name\"]}_metadata.json', 'r'\n",
    "))\n",
    "\n",
    "nb_modalities = torch.load(\n",
    "    f'../datasets/2-preprocessed_data/{config[\"dataset_name\"]}_nb_modalities.pkl',\n",
    "    weights_only=True\n",
    ")\n",
    "\n",
    "for i_fold in range(5):\n",
    "    train_df = pd.read_csv(\n",
    "        f'../datasets/2-preprocessed_data/{config[\"dataset_name\"]}_train_{i_fold}.csv',\n",
    "        encoding='utf-8',\n",
    "        dtype={'student_id': int, 'item_id': int, \"correct\": float, \"dimension_id\": int}\n",
    "    )\n",
    "    valid_df = pd.read_csv(\n",
    "        f'../datasets/2-preprocessed_data/{config[\"dataset_name\"]}_valid_{i_fold}.csv',\n",
    "        encoding='utf-8',\n",
    "        dtype={'student_id': int, 'item_id': int, \"correct\": float, \"dimension_id\": int}\n",
    "    )\n",
    "    test_df = pd.read_csv(\n",
    "        f'../datasets/2-preprocessed_data/{config[\"dataset_name\"]}_test_{i_fold}.csv',\n",
    "        encoding='utf-8',\n",
    "        dtype={'student_id': int, 'item_id': int, \"correct\": float, \"dimension_id\": int}\n",
    "    )\n",
    "\n",
    "    train_data = micat.dataset.CATDataset(train_df, concept_map, metadata, config, nb_modalities)\n",
    "    valid_data = micat.dataset.EvalDataset(valid_df, concept_map, metadata, config, nb_modalities)\n",
    "    test_data  = micat.dataset.EvalDataset(test_df,  concept_map, metadata, config, nb_modalities)\n",
    "\n",
    "    S = micat.selectionStrategy.Random(train_data.metadata, **config)\n",
    "    S.init_models(train_data, valid_data)\n",
    "\n",
    "    # — Training time measurement\n",
    "    logging.info(\"⏳ Training started...\")\n",
    "    t_start = time.time()\n",
    "    S.train(train_data, valid_data)\n",
    "    t_end = time.time()\n",
    "    logging.info(f\"✅ Training completed in {t_end - t_start:.2f} seconds.\")\n",
    "\n",
    "    S.reset_rng()\n",
    "    d = S.evaluate_test(test_data, train_data, valid_data)\n",
    "    logging.info(f\"Fold {config['i_fold']}; results: {d}\")\n",
    "\n",
    "    del S\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fbcc27",
   "metadata": {},
   "source": [
    "### 2. **Figure 2**: Critical Difference Diagram ranking MICAT performances and interpretability with competitors\n",
    "\n",
    "Generate latex of the diagram in 'MICAT/experiments/data/critical_diff.tex'\n",
    "\n",
    "### (to run only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fadaca03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Found 31 candidate result files.\n",
      "Critical difference diagram saved to ../data/critical_diff.tex\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import ast\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "algorithms = [\"Adam\", \"MAML\", \"Approx_GAP\", \"Beta_cd\", \"MICAT\"]\n",
    "datasets = [\"algebra\", \"assist0910\", \"math2\"]\n",
    "metrics = [\"mi_acc\",\"rmse\",\"mae\",\"mi_prec\",\"mi_rec\",\"mi_f_b\",\"mi_auc\",\"ma_prec\",\"ma_rec\",\"ma_f_b\",\"meta_doa\",\"pc-er\",\"rm\"]\n",
    "\n",
    "pattern = re.compile(\n",
    "    r\"^CAT_launch_\"\n",
    "    r\"(?P<dataset>.+?)_\"\n",
    "    r\"(?P<subalgo>.+?)_\"\n",
    "    r\"(?P<algorithm>.+?)_\"\n",
    "    r\"\\d+_all_results\\.json$\"\n",
    ")\n",
    "\n",
    "output_dir = \"cd_plots\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def is_finite(x):\n",
    "    try:\n",
    "        return math.isfinite(float(x))\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def parse_dict_from_text(txt):\n",
    "    m = re.search(r\"[Rr]esults?:\\s*\\(\\s*({.*?})\\s*,\\s*({.*?})\\s*\\)\", txt, re.DOTALL)\n",
    "    if not m:\n",
    "        return {}, {}\n",
    "    def _eval(s):\n",
    "        s = s.replace(\"np.float64\", \"\").replace(\"np.int64\", \"\").replace(\"nan\", \"None\")\n",
    "        try:\n",
    "            return ast.literal_eval(s)\n",
    "        except Exception:\n",
    "            return {}\n",
    "    d1 = _eval(m.group(1))\n",
    "    d2 = _eval(m.group(2))\n",
    "    merged = {}\n",
    "    for d in (d1, d2):\n",
    "        for k, v in (d or {}).items():\n",
    "            if is_finite(v):\n",
    "                merged[k] = float(v)\n",
    "    return merged\n",
    "\n",
    "def load_cdm_basis(dataset, subalgo):\n",
    "    pattern_basis = f\"../scripts/logs/CAT_launch_{dataset}_{subalgo}_CDM_basis_*_*.out\"\n",
    "    files_basis = glob.glob(pattern_basis)\n",
    "    if not files_basis:\n",
    "        print(f\"[WARN] No CDM_basis found for {dataset}/{subalgo}\")\n",
    "        return {}, {}\n",
    "    per_fold = {}\n",
    "    per_metric = {}\n",
    "    for fb in files_basis:\n",
    "        m = re.search(r\"_(\\d+)\\.out$\", fb)\n",
    "        if not m:\n",
    "            continue\n",
    "        fold = int(m.group(1))\n",
    "        with open(fb, \"r\") as f:\n",
    "            txt = f.read()\n",
    "        merged = parse_dict_from_text(txt)\n",
    "        per_fold[fold] = merged\n",
    "        for k, v in merged.items():\n",
    "            per_metric.setdefault(k, []).append(v)\n",
    "    mean_metrics = {k: np.mean(v) for k, v in per_metric.items() if v}\n",
    "    return per_fold, mean_metrics\n",
    "\n",
    "results = {m: {a: [] for a in algorithms} for m in metrics}\n",
    "files = glob.glob(\"../scripts/logs/CAT_launch_*_IMPACT_*_all_results.json\") + \\\n",
    "        glob.glob(\"../scripts/logs/CAT_launch_*_NCDM_*_all_results.json\")\n",
    "print(f\"[INFO] Found {len(files)} candidate result files.\")\n",
    "\n",
    "for file in files:\n",
    "    m = pattern.match(os.path.basename(file))\n",
    "    if not m:\n",
    "        print(f\"[WARN] Filename does not match expected pattern: {file}\")\n",
    "        continue\n",
    "    ds, subalgo, algo = m[\"dataset\"], m[\"subalgo\"], m[\"algorithm\"]\n",
    "    if ds not in datasets or algo not in algorithms:\n",
    "        continue\n",
    "    if ds == \"algebra\" and subalgo == \"NCDM\":\n",
    "        continue\n",
    "\n",
    "    basis_per_fold, basis_mean = load_cdm_basis(ds, subalgo)\n",
    "    if not basis_per_fold:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with open(file, \"r\") as f:\n",
    "            folds = json.load(f)\n",
    "\n",
    "        for fold_idx, fold_data in folds.items():\n",
    "            fold_idx = int(fold_idx)\n",
    "            value_acc = {metric: 0.0 for metric in metrics}\n",
    "\n",
    "            # Each fold_data has {0: predictive, 1: interpretability}\n",
    "            for pred_meta_idx, step_dict in fold_data.items():\n",
    "                for step, metric_dict in step_dict.items():\n",
    "                    for metric in metrics:\n",
    "                        value = metric_dict.get(metric)\n",
    "                        if value is None or np.isnan(value):\n",
    "                            continue\n",
    "                        # Apply CDM basis correction if available\n",
    "                        if (\n",
    "                            metric in basis_mean\n",
    "                            and fold_idx in basis_per_fold\n",
    "                            and metric in basis_per_fold[fold_idx]\n",
    "                        ):\n",
    "                            base_adj = basis_mean[metric] - basis_per_fold[fold_idx][metric]\n",
    "                            value = float(value) + base_adj\n",
    "                        value_acc[metric] += value\n",
    "\n",
    "            for metric in metrics:\n",
    "                results[metric][algo].append(value_acc[metric])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to process {file}: {repr(e)}\")\n",
    "\n",
    "\n",
    "\n",
    "from critdd import Diagrams # Diagrams is the 2D version of Diagram\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# construct a sequence of CD diagrams\n",
    "treatment_names = [\"Naïve\", \"MAML\", \"Approx-GAP\", \"BETA-CD\", \"MICAT\"]\n",
    "metrics = [\"mi_acc\",\"mi_prec\",\"mi_rec\",\"mi_f_b\",\"mi_auc\",'meta_doa']\n",
    "diagram_names = [\"Accuracy\",\"Precision\",\"Recall\",\"F1\",\"AUC\",'Meta Doa']\n",
    "Xs = [] # collect an (n,k)-shaped matrix for each diagram\n",
    "for n in metrics:\n",
    "    Xs.append(np.array([results[n][algo] for algo in algorithms]).T)\n",
    "\n",
    "two_dimensional_diagram = Diagrams(\n",
    "    np.stack(Xs),\n",
    "    diagram_names = diagram_names,\n",
    "    treatment_names = treatment_names,\n",
    "    maximize_outcome = True\n",
    ")\n",
    "\n",
    "two_dimensional_diagram.to_file(\n",
    "    \"../data/critical_diff.tex\",\n",
    "    alpha = .05,\n",
    "    adjustment = \"holm\",\n",
    "    reverse_x = True,\n",
    "    axis_options = {\"title\": \"\"},\n",
    ")\n",
    "print(\"Critical difference diagram saved to ../data/critical_diff.tex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ce3299",
   "metadata": {},
   "source": [
    "### 3. **Figure 3**: Visualization of student proficiency estimate during Meta-testing procedure with MICAT and CD-BPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d13002f9-5c4f-473a-80a7-f5e916327d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05b3b3c-2148-4fa2-9da4-c5027647da5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\"\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":4096:8\"\n",
    "\n",
    "import micat\n",
    "import logging\n",
    "import gc\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from micat.dataset import preprocessing_utilities as pu\n",
    "import argparse\n",
    "import warnings\n",
    "import time  # ← Added to measure time\n",
    "\n",
    "dataset_name = \"math2\"\n",
    "cdm=\"impact\"\n",
    "i_fold=3\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Using device: {device}\")\n",
    "\n",
    "config = micat.utils.generate_eval_config(\n",
    "    CDM=cdm,\n",
    "    dataset_name=dataset_name,\n",
    "    i_fold = i_fold,\n",
    "    load_params=True,\n",
    "    save_params=False,\n",
    "    esc = 'error',\n",
    "    learning_rate = 0.001,\n",
    "    batch_size = 512,\n",
    "    valid_batch_size = 10000,\n",
    "    num_epochs=100,\n",
    "    eval_freq = 1,\n",
    "    patience = 20,\n",
    "    device = device,\n",
    "    pred_metrics = [\"mi_acc\",\"rmse\",\"mae\",\"mi_prec\",\"mi_rec\",\"mi_f_b\",\"mi_auc\",\"ma_prec\",\"ma_rec\",\"ma_f_b\"],\n",
    "    profile_metrics = ['meta_doa','pc-er','rm'],\n",
    "    meta_trainer='GAP',\n",
    "    valid_metric= 'rmse',\n",
    "    n_query=16,\n",
    "    num_inner_users_epochs=3,\n",
    "    lambda_=2.6180638633142202e-05,\n",
    "    inner_user_lr=0.0030952659036283066,\n",
    "    meta_lr=0.01\n",
    ")\n",
    "config['learning_users_emb_lr'] = 0.001\n",
    "logging.info(f'#### config : {config} ####')\n",
    "\n",
    "concept_map = json.load(open(\n",
    "    f'../datasets/2-preprocessed_data/{config[\"dataset_name\"]}_concept_map.json', 'r'\n",
    "))\n",
    "concept_map = {int(k): [int(x) for x in v] for k, v in concept_map.items()}\n",
    "\n",
    "metadata = json.load(open(\n",
    "    f'../datasets/2-preprocessed_data/{config[\"dataset_name\"]}_metadata.json', 'r'\n",
    "))\n",
    "\n",
    "nb_modalities = torch.load(\n",
    "    f'../datasets/2-preprocessed_data/{config[\"dataset_name\"]}_nb_modalities.pkl',\n",
    "    weights_only=True\n",
    ")\n",
    "\n",
    "train_df = pd.read_csv(\n",
    "    f'../datasets/2-preprocessed_data/{config[\"dataset_name\"]}_train_{i_fold}.csv',\n",
    "    encoding='utf-8',\n",
    "    dtype={'student_id': int, 'item_id': int, \"correct\": float, \"dimension_id\": int}\n",
    ")\n",
    "valid_df = pd.read_csv(\n",
    "    f'../datasets/2-preprocessed_data/{config[\"dataset_name\"]}_valid_{i_fold}.csv',\n",
    "    encoding='utf-8',\n",
    "    dtype={'student_id': int, 'item_id': int, \"correct\": float, \"dimension_id\": int}\n",
    ")\n",
    "test_df = pd.read_csv(\n",
    "    f'../datasets/2-preprocessed_data/{config[\"dataset_name\"]}_test_{i_fold}.csv',\n",
    "    encoding='utf-8',\n",
    "    dtype={'student_id': int, 'item_id': int, \"correct\": float, \"dimension_id\": int}\n",
    ")\n",
    "\n",
    "train_data = micat.dataset.CATDataset(train_df, concept_map, metadata, config, nb_modalities)\n",
    "valid_data = micat.dataset.EvalDataset(valid_df, concept_map, metadata, config, nb_modalities)\n",
    "test_data  = micat.dataset.EvalDataset(test_df,  concept_map, metadata, config, nb_modalities)\n",
    "\n",
    "S = micat.selectionStrategy.Random(train_data.metadata, **config)\n",
    "S.init_models(train_data, valid_data)\n",
    "\n",
    "test_emb = torch.load(f'../data/evolving_emb_tensor_MICAT_{i_fold}.pt',weights_only=True).cpu()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d745c6cc-1972-44ff-8fb2-1f03b6e92cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_emb = S.CDM.model.users_emb.weight#torch.load(f'../data/train_emb_tensor_MICAT_{i_fold}.pt',weights_only=False).weight.cpu()\n",
    "q_ids=torch.load(f'../data/_sub_question_ids_MICAT_3.pt').cpu()\n",
    "u_ids=torch.load(f'../data/_sub_user_ids_MICAT_3.pt').cpu()\n",
    "l_ids=torch.load(f'../data/_sub_labels_MICAT_3.pt').cpu()\n",
    "#c_ids=torch.load(f'../data/_sub_categories_MICAT_3.pt').cpu()\n",
    "\n",
    "train_users_ids = torch.Tensor(list(train_data.users_id)).long()\n",
    "train_emb = train_emb[train_users_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5fc698-494a-45fd-8513-5a8edfa1aaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pu.plot_embedding_distribution(2,13,train_emb.detach().cpu().numpy(),test_emb, test_data=test_data, train_emb=train_emb, q_ids=q_ids, u_ids=u_ids,l_ids=l_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92fcbfc-d9b9-4338-898f-09cbc6a5368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1) : \n",
    "    pu.plot_embedding_distribution_comp(i,i+1,train_emb.detach().cpu().numpy(), test_emb, test_data=test_data,train_emb=train_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c5d9b3-639f-4ef0-ab4c-382c38cd626f",
   "metadata": {},
   "source": [
    "### 4. **Figure 4**: ACC vs Meta-DOA curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3d6495-8dc9-45cf-ab3d-22aab301d212",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os, re, csv, sys, argparse, math, glob\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460bcd72-5459-40af-92e2-c9531e5e1f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, metrics_seen = build_results(\"../scripts/logs/\", target_metric=\"mi_acc\", verbose=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeb2810-1634-48e4-98da-567d25b77bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_seen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.8.0_py3.12.11",
   "language": "python",
   "name": "module-conda-env-pytorch-gpu-2.8.0_py3.12.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}